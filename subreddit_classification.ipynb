{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kFbi-mEZeiHN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "import glob\n",
        "from glob import glob\n",
        "import pathlib\n",
        "import csv\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Conv1D, MaxPooling1D, Dense, Input, Dropout, GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/\" # Using colab"
      ],
      "metadata": {
        "id": "_nXxOB9U0aJn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data (merging fine grained data .csv files(train, test, val) from link, 17 subreddits)\n",
        "files = glob(os.path.join(path, \"*.csv\"))\n",
        "dataframe = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)"
      ],
      "metadata": {
        "id": "wHe-nCV86uux"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_ad1JW77IdE",
        "outputId": "3d9f8890-77b5-41ee-d310-d0c09c8f6eae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25500, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = dataframe.sort_values(by=['label'])"
      ],
      "metadata": {
        "id": "taaS3N6R3QqX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8vccyLIY4Jra",
        "outputId": "f6f4dbe8-6438-480d-dcbe-d14fc7d99246"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           label                                               text\n",
              "15492  AskReddit  How much do you like or dislike girls who alwa...\n",
              "12205  AskReddit  What's a 10/10 album from the last 15 years by...\n",
              "20637  AskReddit               Whats the most racist joke you know?\n",
              "1401   AskReddit  I find gauges to be unattractive and stupid. R...\n",
              "12190  AskReddit  Anyone who won a \"lifetime supply\" of somethin..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f445921-cdab-4235-a014-bcdfb4aff144\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15492</th>\n",
              "      <td>AskReddit</td>\n",
              "      <td>How much do you like or dislike girls who alwa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12205</th>\n",
              "      <td>AskReddit</td>\n",
              "      <td>What's a 10/10 album from the last 15 years by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20637</th>\n",
              "      <td>AskReddit</td>\n",
              "      <td>Whats the most racist joke you know?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1401</th>\n",
              "      <td>AskReddit</td>\n",
              "      <td>I find gauges to be unattractive and stupid. R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12190</th>\n",
              "      <td>AskReddit</td>\n",
              "      <td>Anyone who won a \"lifetime supply\" of somethin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f445921-cdab-4235-a014-bcdfb4aff144')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f445921-cdab-4235-a014-bcdfb4aff144 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f445921-cdab-4235-a014-bcdfb4aff144');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = -1\n",
        "label_names = []\n",
        "labels = []\n",
        "features = []\n",
        "for index, row in dataframe.iterrows():\n",
        "  if row['label'] not in label_names:\n",
        "    label_names.append(row['label'])\n",
        "    c+=1\n",
        "  labels.append(c)\n",
        "  features.append(row['text'])\n",
        "print(\"There are %d samples\" % (len(features)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCud2fvl2svn",
        "outputId": "c0530003-3fa6-4a2a-8d19-e0c6f35e5a23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 25500 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"And there are %d different subreddits\" % len(label_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss7tjB1k51I-",
        "outputId": "a462e886-f602-4f1e-e34d-fdc5af4f6f7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And there are 17 different subreddits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1219\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(features)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "validation_split = 0.2\n",
        "num_test_features = int(validation_split * len(features))\n",
        "train_x = features[:-num_test_features]\n",
        "test_x = features[-num_test_features:]\n",
        "train_y = labels[:-num_test_features]\n",
        "test_y = labels[-num_test_features:]"
      ],
      "metadata": {
        "id": "UvRVmP2BgkLj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_x))\n",
        "print(len(test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gi0cf1T-PkU",
        "outputId": "77302752-24e5-49ac-d934-1ac23bddfdce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5100\n",
            "5100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorization(max_tokens=25000, output_sequence_length=150)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_x).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "F-gZq3n8gp95"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))"
      ],
      "metadata": {
        "id": "e4NPZEBHg1l2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Li1wmyWg3wR",
        "outputId": "d10eee01-aaab-4a32-e34f-fe462df30b34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 03:01:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-03-21 03:01:35--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-03-21 03:01:35--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2023-03-21 03:04:15 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code from Keras\n",
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \"/content/glove.6B.100d.txt\"\n",
        ")\n",
        "a = 0\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    if(a<10):\n",
        "      print(line)\n",
        "      a+=1\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiiwQ2Pzg7J3",
        "outputId": "35d42a89-14f1-47c8-8bc5-62571c06cf20"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
            "\n",
            ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
            "\n",
            ". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n",
            "\n",
            "of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n",
            "\n",
            "to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n",
            "\n",
            "and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\n",
            "\n",
            "in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\n",
            "\n",
            "a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\n",
            "\n",
            "\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\n",
            "\n",
            "'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\n",
            "\n",
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code from Keras\n",
        "num_tokens = len(vocabulary) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)(meaning that the words were not in the embedding matrix\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEQBLo-Pg-va",
        "outputId": "c432cd0c-9898-4484-c622-b522119a42b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 20328 words (4672 misses)(meaning that the words were not in the embedding matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "WqxdAPoVhVA4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Keras\n",
        "input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(input)\n",
        "X = layers.Conv1D(128, 5, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(embedded_sequences)\n",
        "X = layers.MaxPooling1D(5)(X)\n",
        "X = Dropout(0.4)(X)\n",
        "X = layers.Conv1D(128, 5, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(X)\n",
        "X = layers.GlobalMaxPooling1D()(X)\n",
        "X = Dropout(0.4)(X)\n",
        "X = layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(X)\n",
        "X = layers.Dropout(0.4)(X)\n",
        "output = layers.Dense(len(label_names), activation=\"softmax\")(X)\n",
        "model = keras.Model(input, output)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBGZpX_KhWqr",
        "outputId": "3d11a1c5-538d-456b-8a66-07378a92335c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         2500200   \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, None, 128)         64128     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, None, 128)         0         \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 17)                2193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,665,081\n",
            "Trainable params: 164,881\n",
            "Non-trainable params: 2,500,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_x])).numpy()\n",
        "x_test = vectorizer(np.array([[s] for s in test_x])).numpy()\n",
        "\n",
        "y_train = np.array(train_y).reshape(20400,1)\n",
        "y_test = np.array(test_y).reshape(5100,1)"
      ],
      "metadata": {
        "id": "PNau81_bhY9b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSSDgZCKrd4Z",
        "outputId": "10c8e7d5-c90d-4a1d-e307-d5ae06763f26"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "model.fit(x=x_train, y=y_train, batch_size=128, epochs=80, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNcTP9qxhaH5",
        "outputId": "ce95cbae-cd05-4f60-95b8-d215780d5217"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "160/160 [==============================] - 3s 7ms/step - loss: 2.7106 - acc: 0.2297 - val_loss: 2.0535 - val_acc: 0.4459\n",
            "Epoch 2/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.9620 - acc: 0.4568 - val_loss: 1.6692 - val_acc: 0.5751\n",
            "Epoch 3/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.6790 - acc: 0.5571 - val_loss: 1.5241 - val_acc: 0.6102\n",
            "Epoch 4/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.5384 - acc: 0.5984 - val_loss: 1.4574 - val_acc: 0.6306\n",
            "Epoch 5/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.4412 - acc: 0.6304 - val_loss: 1.4073 - val_acc: 0.6441\n",
            "Epoch 6/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.3800 - acc: 0.6504 - val_loss: 1.3959 - val_acc: 0.6518\n",
            "Epoch 7/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.3229 - acc: 0.6693 - val_loss: 1.3758 - val_acc: 0.6584\n",
            "Epoch 8/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.2856 - acc: 0.6855 - val_loss: 1.3522 - val_acc: 0.6665\n",
            "Epoch 9/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.2455 - acc: 0.6958 - val_loss: 1.3587 - val_acc: 0.6684\n",
            "Epoch 10/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.2191 - acc: 0.7014 - val_loss: 1.3726 - val_acc: 0.6722\n",
            "Epoch 11/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.1907 - acc: 0.7141 - val_loss: 1.3659 - val_acc: 0.6669\n",
            "Epoch 12/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.1709 - acc: 0.7225 - val_loss: 1.3538 - val_acc: 0.6743\n",
            "Epoch 13/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.1541 - acc: 0.7257 - val_loss: 1.3665 - val_acc: 0.6739\n",
            "Epoch 14/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.1328 - acc: 0.7323 - val_loss: 1.3578 - val_acc: 0.6722\n",
            "Epoch 15/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.1115 - acc: 0.7406 - val_loss: 1.3798 - val_acc: 0.6678\n",
            "Epoch 16/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0982 - acc: 0.7484 - val_loss: 1.3845 - val_acc: 0.6727\n",
            "Epoch 17/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0766 - acc: 0.7557 - val_loss: 1.3934 - val_acc: 0.6688\n",
            "Epoch 18/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0577 - acc: 0.7601 - val_loss: 1.3982 - val_acc: 0.6747\n",
            "Epoch 19/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0694 - acc: 0.7566 - val_loss: 1.4037 - val_acc: 0.6706\n",
            "Epoch 20/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0577 - acc: 0.7614 - val_loss: 1.3755 - val_acc: 0.6759\n",
            "Epoch 21/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0372 - acc: 0.7708 - val_loss: 1.3910 - val_acc: 0.6714\n",
            "Epoch 22/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0226 - acc: 0.7760 - val_loss: 1.4133 - val_acc: 0.6739\n",
            "Epoch 23/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0135 - acc: 0.7795 - val_loss: 1.3999 - val_acc: 0.6780\n",
            "Epoch 24/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 1.0064 - acc: 0.7792 - val_loss: 1.4479 - val_acc: 0.6773\n",
            "Epoch 25/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9983 - acc: 0.7850 - val_loss: 1.3979 - val_acc: 0.6804\n",
            "Epoch 26/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9800 - acc: 0.7905 - val_loss: 1.4268 - val_acc: 0.6688\n",
            "Epoch 27/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9892 - acc: 0.7851 - val_loss: 1.4350 - val_acc: 0.6686\n",
            "Epoch 28/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9750 - acc: 0.7931 - val_loss: 1.4002 - val_acc: 0.6739\n",
            "Epoch 29/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9677 - acc: 0.7964 - val_loss: 1.4438 - val_acc: 0.6678\n",
            "Epoch 30/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9640 - acc: 0.7968 - val_loss: 1.4684 - val_acc: 0.6671\n",
            "Epoch 31/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9519 - acc: 0.7996 - val_loss: 1.5179 - val_acc: 0.6616\n",
            "Epoch 32/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9508 - acc: 0.8003 - val_loss: 1.4250 - val_acc: 0.6753\n",
            "Epoch 33/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9453 - acc: 0.8036 - val_loss: 1.4340 - val_acc: 0.6737\n",
            "Epoch 34/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9395 - acc: 0.8028 - val_loss: 1.4473 - val_acc: 0.6722\n",
            "Epoch 35/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9285 - acc: 0.8075 - val_loss: 1.4512 - val_acc: 0.6780\n",
            "Epoch 36/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9350 - acc: 0.8023 - val_loss: 1.4457 - val_acc: 0.6761\n",
            "Epoch 37/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9253 - acc: 0.8096 - val_loss: 1.4500 - val_acc: 0.6759\n",
            "Epoch 38/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9202 - acc: 0.8100 - val_loss: 1.4746 - val_acc: 0.6729\n",
            "Epoch 39/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9166 - acc: 0.8120 - val_loss: 1.4691 - val_acc: 0.6702\n",
            "Epoch 40/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9139 - acc: 0.8156 - val_loss: 1.4734 - val_acc: 0.6718\n",
            "Epoch 41/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9061 - acc: 0.8178 - val_loss: 1.4817 - val_acc: 0.6757\n",
            "Epoch 42/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9054 - acc: 0.8211 - val_loss: 1.4926 - val_acc: 0.6708\n",
            "Epoch 43/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8993 - acc: 0.8229 - val_loss: 1.4609 - val_acc: 0.6749\n",
            "Epoch 44/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.9083 - acc: 0.8150 - val_loss: 1.4752 - val_acc: 0.6722\n",
            "Epoch 45/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8994 - acc: 0.8234 - val_loss: 1.4781 - val_acc: 0.6698\n",
            "Epoch 46/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8842 - acc: 0.8236 - val_loss: 1.4689 - val_acc: 0.6776\n",
            "Epoch 47/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8853 - acc: 0.8252 - val_loss: 1.4718 - val_acc: 0.6727\n",
            "Epoch 48/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8813 - acc: 0.8277 - val_loss: 1.4913 - val_acc: 0.6694\n",
            "Epoch 49/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8779 - acc: 0.8261 - val_loss: 1.4766 - val_acc: 0.6755\n",
            "Epoch 50/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8693 - acc: 0.8324 - val_loss: 1.4991 - val_acc: 0.6725\n",
            "Epoch 51/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8747 - acc: 0.8283 - val_loss: 1.4929 - val_acc: 0.6718\n",
            "Epoch 52/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8866 - acc: 0.8270 - val_loss: 1.5179 - val_acc: 0.6684\n",
            "Epoch 53/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8725 - acc: 0.8298 - val_loss: 1.5187 - val_acc: 0.6684\n",
            "Epoch 54/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8728 - acc: 0.8287 - val_loss: 1.4975 - val_acc: 0.6743\n",
            "Epoch 55/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8600 - acc: 0.8352 - val_loss: 1.5073 - val_acc: 0.6688\n",
            "Epoch 56/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8567 - acc: 0.8353 - val_loss: 1.5277 - val_acc: 0.6669\n",
            "Epoch 57/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8481 - acc: 0.8389 - val_loss: 1.5231 - val_acc: 0.6614\n",
            "Epoch 58/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8501 - acc: 0.8355 - val_loss: 1.5422 - val_acc: 0.6657\n",
            "Epoch 59/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8587 - acc: 0.8343 - val_loss: 1.5487 - val_acc: 0.6647\n",
            "Epoch 60/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8549 - acc: 0.8367 - val_loss: 1.5120 - val_acc: 0.6675\n",
            "Epoch 61/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8575 - acc: 0.8334 - val_loss: 1.5237 - val_acc: 0.6696\n",
            "Epoch 62/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8499 - acc: 0.8381 - val_loss: 1.5443 - val_acc: 0.6692\n",
            "Epoch 63/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8375 - acc: 0.8415 - val_loss: 1.5407 - val_acc: 0.6682\n",
            "Epoch 64/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8330 - acc: 0.8418 - val_loss: 1.5269 - val_acc: 0.6631\n",
            "Epoch 65/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8368 - acc: 0.8409 - val_loss: 1.5213 - val_acc: 0.6698\n",
            "Epoch 66/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8344 - acc: 0.8434 - val_loss: 1.5222 - val_acc: 0.6667\n",
            "Epoch 67/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8371 - acc: 0.8395 - val_loss: 1.5236 - val_acc: 0.6694\n",
            "Epoch 68/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8298 - acc: 0.8422 - val_loss: 1.5341 - val_acc: 0.6667\n",
            "Epoch 69/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8233 - acc: 0.8458 - val_loss: 1.5658 - val_acc: 0.6610\n",
            "Epoch 70/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8319 - acc: 0.8440 - val_loss: 1.5509 - val_acc: 0.6649\n",
            "Epoch 71/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8384 - acc: 0.8400 - val_loss: 1.5433 - val_acc: 0.6733\n",
            "Epoch 72/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8260 - acc: 0.8465 - val_loss: 1.5498 - val_acc: 0.6694\n",
            "Epoch 73/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8378 - acc: 0.8440 - val_loss: 1.5413 - val_acc: 0.6661\n",
            "Epoch 74/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8253 - acc: 0.8474 - val_loss: 1.5432 - val_acc: 0.6684\n",
            "Epoch 75/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8228 - acc: 0.8475 - val_loss: 1.5590 - val_acc: 0.6625\n",
            "Epoch 76/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8216 - acc: 0.8465 - val_loss: 1.5382 - val_acc: 0.6733\n",
            "Epoch 77/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8164 - acc: 0.8505 - val_loss: 1.5485 - val_acc: 0.6702\n",
            "Epoch 78/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8098 - acc: 0.8473 - val_loss: 1.5398 - val_acc: 0.6631\n",
            "Epoch 79/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8148 - acc: 0.8499 - val_loss: 1.5980 - val_acc: 0.6598\n",
            "Epoch 80/80\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.8141 - acc: 0.8499 - val_loss: 1.5965 - val_acc: 0.6594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee9458dd00>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGsZil-qrOO6",
        "outputId": "9070e0b4-5e00-4d14-d64f-05249392e43e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 150)\n",
            "(5100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hxdt_JAkg8-",
        "outputId": "5cf802da-49fc-4eab-a764-e65c4a39e898"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 3ms/step - loss: 1.5965 - acc: 0.6592\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5964751243591309, 0.6592156887054443]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = model(x)\n",
        "fin_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = fin_model.predict([[\"What famous person didn't deserve all the hate that they got?\"]]) # Post taken directly from AskReddit\n",
        "\n",
        "label_names[np.argmax(probabilities[0])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0zPTdaIthcMs",
        "outputId": "5624f100-23bd-4d72-ac31-4efdba178b5a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 142ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AskReddit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux6xnkF-iOKV",
        "outputId": "4216829b-70c3-43a1-de60-87f51faa33a1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AskReddit', 'Futurology', 'Jokes', 'Showerthoughts', 'WritingPrompts', 'askscience', 'bestof', 'explainlikeimfive', 'history', 'nosleep', 'personalfinance', 'politics', 'science', 'television', 'todayilearned', 'videos', 'worldnews']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Done!"
      ],
      "metadata": {
        "id": "X_8sIOYfgVt3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}